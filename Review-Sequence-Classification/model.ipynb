{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd574536",
   "metadata": {},
   "source": [
    "Install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4bbb2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryler/Programming/Projects/Custom-Models/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2dcebf",
   "metadata": {},
   "source": [
    "Load in the dataset with latin1 encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad75a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/ryler/Datasets/Mcdonalds-Review-Text-Classification/McDonald_s_Reviews.csv\", encoding=\"latin1\")\n",
    "# df = pd.read_csv(\"/home/rynutty/Documents/DataSets/Mcdonalds-Reviews/McDonald_s_Reviews.csv\", encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41a2c61",
   "metadata": {},
   "source": [
    "Review the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e89950f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>store_name</th>\n",
       "      <th>category</th>\n",
       "      <th>store_address</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>review_time</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>3 months ago</td>\n",
       "      <td>Why does it look like someone spit on my food?...</td>\n",
       "      <td>1 star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>It'd McDonalds. It is what it is as far as the...</td>\n",
       "      <td>4 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Made a mobile order got to the speaker and che...</td>\n",
       "      <td>1 star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>a month ago</td>\n",
       "      <td>My mc. Crispy chicken sandwich was ï¿½ï¿½ï¿½ï¿...</td>\n",
       "      <td>5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>I repeat my order 3 times in the drive thru, a...</td>\n",
       "      <td>1 star</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewer_id  store_name              category  \\\n",
       "0            1  McDonald's  Fast food restaurant   \n",
       "1            2  McDonald's  Fast food restaurant   \n",
       "2            3  McDonald's  Fast food restaurant   \n",
       "3            4  McDonald's  Fast food restaurant   \n",
       "4            5  McDonald's  Fast food restaurant   \n",
       "\n",
       "                                       store_address  latitude   longitude  \\\n",
       "0  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
       "1  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
       "2  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
       "3  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
       "4  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
       "\n",
       "  rating_count   review_time  \\\n",
       "0        1,240  3 months ago   \n",
       "1        1,240    5 days ago   \n",
       "2        1,240    5 days ago   \n",
       "3        1,240   a month ago   \n",
       "4        1,240  2 months ago   \n",
       "\n",
       "                                              review   rating  \n",
       "0  Why does it look like someone spit on my food?...   1 star  \n",
       "1  It'd McDonalds. It is what it is as far as the...  4 stars  \n",
       "2  Made a mobile order got to the speaker and che...   1 star  \n",
       "3  My mc. Crispy chicken sandwich was ï¿½ï¿½ï¿½ï¿...  5 stars  \n",
       "4  I repeat my order 3 times in the drive thru, a...   1 star  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edc1026",
   "metadata": {},
   "source": [
    "Get a sense of the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2dc8696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33396, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad6301f",
   "metadata": {},
   "source": [
    "I saw someone else modify the ratings label into three classes. 2 = positive rating, 1 = neutral rating, 0 = negative rating. I think this makes a lot more sense than predicting exact start ratings because it's never implied in a review whether the rating would be 4 or 5 starts, but it's easy to tell whether the rating is positive, neutral, or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f2cc6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ratings(ratings):\n",
    "    ratings = [int(rate[0]) for rate in ratings]\n",
    "    cleaned_ratings = []\n",
    "\n",
    "    for rating in ratings:\n",
    "        if rating >= 4:\n",
    "            cleaned_ratings.append(2)\n",
    "        elif rating == 3:\n",
    "            cleaned_ratings.append(1)\n",
    "        else:\n",
    "            cleaned_ratings.append(0)\n",
    "\n",
    "    return cleaned_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26575a",
   "metadata": {},
   "source": [
    "Split our dataset into evaluation and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79a18ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cutoff = int(len(df) * 0.8)\n",
    "\n",
    "train = df.iloc[:train_cutoff, :]\n",
    "eval = df.iloc[train_cutoff:, :].reset_index(drop=True)\n",
    "\n",
    "x_train = train[\"review\"]\n",
    "y_train = clean_ratings(train[\"rating\"])\n",
    "\n",
    "x_eval = eval[\"review\"]\n",
    "y_eval = clean_ratings(eval[\"rating\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8719e858",
   "metadata": {},
   "source": [
    "MLP class just to make the code a little bit easier to read. Not very necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab333d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.inference = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inference(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6df67",
   "metadata": {},
   "source": [
    "This is where I made some transformer encoder blocks for contextualising our embeddings, this is what actually processes the data.\n",
    "\n",
    "The embeddings start off carrying no contextual information, each token only knows about itself. Transformers use a mechanism known as Attention. Attention makes each word in the sentence look at eachother, for example the last word in a sentence may capture the meaning of the entire sentence because it now carries information about all of the words that came before it.\n",
    "\n",
    "Think of how you would read a paragraph, you start at the beginning knowing nothing, but by the end you've built up enough context so that by the time you reach the last word, you understand the meaning behind the paragraph, that's similar to how transformers behave for NLP tasks.\n",
    "\n",
    "And once you have that embedding that carries the whole meaning of a sentence or paragraph, you can filter that through a MLP to get a good classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6be47542",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, num_encoder_blocks):\n",
    "        super().__init__()\n",
    "\n",
    "        blocks = [EncoderBlock(d_model=d_model, num_heads=num_heads) for _ in range(num_encoder_blocks)]\n",
    "        self.inference = nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inference(x)\n",
    "    \n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=0.2, batch_first=True, device=\"cuda\")\n",
    "        self.mlp = MLP(in_features=d_model, out_features=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        identity = x\n",
    "        x, attention_weights = self.mha(x, x, x)\n",
    "        x = x + identity\n",
    "        x = self.layer_norm(x)\n",
    "        identity = x\n",
    "        x = self.mlp(x)\n",
    "        x = x + identity\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c542368",
   "metadata": {},
   "source": [
    "This model just ties together my encoder with a classification head that takes the contextualised embeddings and makes classification with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33539b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.encoder = Encoder(d_model=768, num_heads=12, num_encoder_blocks=2)\n",
    "        self.head = nn.Linear(in_features=768, out_features=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ab3a5",
   "metadata": {},
   "source": [
    "I decided to use a pretrained embedding space from bert because I didn't want to take a long time training my own, I didn't let bert process the embeddings, I just got positional encodings and base embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5142715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder():\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def __call__(self, sentences: str):\n",
    "\n",
    "        if isinstance(sentences, torch.Tensor):\n",
    "            sentences = list(sentences)\n",
    "            \n",
    "        input_ids = self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        embeddings = self.model.embeddings(input_ids)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b05dae1",
   "metadata": {},
   "source": [
    "Dataset class for my datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0a87e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "\n",
    "    def __init__(self, reviews, ratings):\n",
    "        super().__init__()\n",
    "\n",
    "        self.reviews = reviews\n",
    "        self.ratings = ratings\n",
    "        self.embedder = Embedder()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.reviews[idx], self.ratings[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e1846",
   "metadata": {},
   "source": [
    "My model initializaton, optimizer initialization, and loss function initialization. You can also see I moved my model to my GPU for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eab83e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequenceClassifier().to(\"cuda\")\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d19b697",
   "metadata": {},
   "source": [
    "Created my data loaders with my dataset class so that training becomes more simple, and it batches for me automatically, it also shuffles too which is nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec43ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ReviewDataset(reviews=x_train, ratings=y_train)\n",
    "test_dataset = ReviewDataset(reviews=x_eval, ratings=y_eval)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba447ee",
   "metadata": {},
   "source": [
    "Clear anything off of my GPU that we don't need so it doesn't mess with training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ded2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2794299",
   "metadata": {},
   "source": [
    "Run the training loop for 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53d50b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:25<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.01979332622227099, accuracy: 0.7558766282377601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:10<00:00, 20.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.01728005724663506, accuracy: 0.7901197604790419\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:26<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.01685548608704034, accuracy: 0.7900509058242252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:10<00:00, 19.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.01689126124817454, accuracy: 0.7925149700598803\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:25<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.015891447748086967, accuracy: 0.8001946399161551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:09<00:00, 21.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.015023360452698377, accuracy: 0.8247005988023952\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:21<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.015306451312304079, accuracy: 0.8072690522533313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:09<00:00, 21.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.014783857747198578, accuracy: 0.8203592814371258\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:22<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.014963731971204165, accuracy: 0.8111618505764336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:09<00:00, 21.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.014278978698267908, accuracy: 0.8252994011976048\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:25<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.014652793409604301, accuracy: 0.8140814493187603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:10<00:00, 19.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.015495571784095136, accuracy: 0.8143712574850299\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:25<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.014437289531202548, accuracy: 0.8176373708639018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:10<00:00, 19.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.014371763437451003, accuracy: 0.8199101796407186\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:25<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.014215425828446402, accuracy: 0.8231022608174876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:10<00:00, 19.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.0145841494581835, accuracy: 0.8194610778443113\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:24<00:00,  9.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.013999511107527972, accuracy: 0.8228776762988471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:10<00:00, 19.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.01410243453424491, accuracy: 0.828443113772455\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:25<00:00,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.01384698737511415, accuracy: 0.825759844288067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:10<00:00, 19.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.014030677207305046, accuracy: 0.8288922155688623\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:25<00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.013626957607561108, accuracy: 0.8277062434496182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:10<00:00, 19.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.014332331254141416, accuracy: 0.8315868263473054\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:25<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.013519834683543463, accuracy: 0.8277811049558317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:10<00:00, 19.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.01366175369409744, accuracy: 0.8345808383233533\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:24<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.013355428243351833, accuracy: 0.8317487647851475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:10<00:00, 20.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.013377636305229392, accuracy: 0.8392215568862276\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:25<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.013218331532080135, accuracy: 0.8353421170833957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:10<00:00, 19.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.013688191314211149, accuracy: 0.834131736526946\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:25<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.013053038635942765, accuracy: 0.8339571792184459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:10<00:00, 20.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.01346589008758882, accuracy: 0.8339820359281437\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:24<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.012912344770359054, accuracy: 0.835154963317862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:10<00:00, 20.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.013326317963084418, accuracy: 0.8407185628742515\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:25<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.012803539453036129, accuracy: 0.8364276089234916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:10<00:00, 20.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.013685762770697028, accuracy: 0.8327844311377246\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:25<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.012694661106581299, accuracy: 0.8384488695912562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:10<00:00, 19.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.01300896715573565, accuracy: 0.8401197604790419\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:24<00:00,  9.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.01262327447965315, accuracy: 0.8398338074562061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:10<00:00, 20.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.013015102334097474, accuracy: 0.8422155688622754\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [01:25<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.012554030250219693, accuracy: 0.8393472076658183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:10<00:00, 20.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.013161573931574822, accuracy: 0.8423652694610778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "embedder = Embedder()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(\"Starting Training...\")\n",
    "\n",
    "    running_train_loss = 0\n",
    "    train_total = 0\n",
    "    train_correct = 0\n",
    "\n",
    "    model.train()\n",
    "    for reviews, ratings in tqdm(train_dataloader):\n",
    "        embeddings = embedder(reviews).to(\"cuda\")\n",
    "        ratings = ratings.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(embeddings)\n",
    "\n",
    "        loss = loss_fn(logits, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "        train_total += len(reviews)\n",
    "        prediction = torch.argmax(logits, dim=1)\n",
    "        train_correct += (prediction == ratings).sum().item()\n",
    "\n",
    "    print(f\"Avg loss: {running_train_loss / train_total}, accuracy: {train_correct / train_total}\")\n",
    "\n",
    "    running_eval_loss = 0\n",
    "    eval_total = 0\n",
    "    eval_correct = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for reviews, ratings in tqdm(test_dataloader):\n",
    "            embeddings = embedder(reviews).to(\"cuda\")\n",
    "            ratings = ratings.to(\"cuda\")\n",
    "\n",
    "            logits = model(embeddings)\n",
    "            loss = loss_fn(logits, ratings)\n",
    "\n",
    "            running_eval_loss += loss.item()\n",
    "            eval_total += len(reviews)\n",
    "            predicted = torch.argmax(logits, dim=1)\n",
    "            eval_correct += (predicted == ratings).sum().item()\n",
    "\n",
    "    print(f\"Avg loss: {running_eval_loss / eval_total}, accuracy: {eval_correct / eval_total}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd02db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f008b72",
   "metadata": {},
   "source": [
    "Thank you for reading through this if you did, this is my first post on kaggle so hopefully it went alright! I also noticed my model didn't end up overfitting which I love to see."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
