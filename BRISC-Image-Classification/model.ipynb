{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7567e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a46abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db8de605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 3, 224, 224])\n",
      "torch.Size([24])\n"
     ]
    }
   ],
   "source": [
    "train_data = ImageFolder(root=\"/home/rynutty/Documents/ProgrammingProjects/CustomModels/datasets/brisc2025/classification_task/train\", transform=transform)\n",
    "train_dataloader = DataLoader(train_data, 24, shuffle=True)\n",
    "\n",
    "images, labels = next(iter(train_dataloader))\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a32382a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "test_data = ImageFolder(\"/home/rynutty/Documents/ProgrammingProjects/CustomModels/datasets/brisc2025/classification_task/test\", transform=transform)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "images, labels = next(iter(test_dataloader))\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97425674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BriscCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=4, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.MaxPool2d(kernel_size=5, stride=2),\n",
    "            nn.Dropout2d(p=0.25)\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.MaxPool2d(kernel_size=5, stride=2),\n",
    "            nn.Dropout2d(p=0.25)\n",
    "        )\n",
    "\n",
    "        dummy_ex = torch.randn((1, 3, 224, 224))\n",
    "        dummy_ex = self.conv_block1(dummy_ex)\n",
    "        dummy_ex = self.conv_block2(dummy_ex)\n",
    "        in_features = dummy_ex.view(-1)\n",
    "        \n",
    "        self.dense = nn.Linear(in_features=in_features.shape[0], out_features=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.dense(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7225d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BriscCNN()\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8b8a3f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1...\n",
      "Avg Epoch Loss: 0.26096274664357766, Accuracy: 0.742\n",
      "Evaluating Epoch 1...\n",
      "Avg Epoch Loss: 0.04607568550109863, Accuracy: 0.807\n",
      "Training Epoch 2...\n",
      "Avg Epoch Loss: 0.09800977948989013, Accuracy: 0.8782\n",
      "Evaluating Epoch 2...\n",
      "Avg Epoch Loss: 0.030572465658187865, Accuracy: 0.863\n",
      "Training Epoch 3...\n",
      "Avg Epoch Loss: 0.04117216487390573, Accuracy: 0.9372\n",
      "Evaluating Epoch 3...\n",
      "Avg Epoch Loss: 0.029527729652822018, Accuracy: 0.881\n",
      "Training Epoch 4...\n",
      "Avg Epoch Loss: 0.030269136799403896, Accuracy: 0.951\n",
      "Evaluating Epoch 4...\n",
      "Avg Epoch Loss: 0.023087321817874908, Accuracy: 0.905\n",
      "Training Epoch 5...\n",
      "Avg Epoch Loss: 0.01917389002093844, Accuracy: 0.9632\n",
      "Evaluating Epoch 5...\n",
      "Avg Epoch Loss: 0.02970124664902687, Accuracy: 0.887\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f\"Training Epoch {epoch}...\")\n",
    "\n",
    "    train_working_loss = 0\n",
    "    train_total = 0\n",
    "    train_correct = 0\n",
    "\n",
    "    model.train()\n",
    "    for image_batch, label_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(image_batch)\n",
    "        loss = loss_fn(logits, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_working_loss += loss.item()\n",
    "        train_total += len(image_batch)\n",
    "        predicted = logits.max(1)\n",
    "        train_correct += (predicted.indices == label_batch).sum().item()\n",
    "\n",
    "    print(f\"Avg Epoch Loss: {train_working_loss / train_total}, Accuracy: {train_correct / train_total}\")\n",
    "    \n",
    "    test_working_loss = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    print(f\"Evaluating Epoch {epoch}...\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image_batch, label_batch in test_dataloader:\n",
    "            logits = model(image_batch)\n",
    "            loss = loss_fn(logits, label_batch)\n",
    "\n",
    "            test_working_loss += loss.item()\n",
    "            test_total += len(image_batch)\n",
    "            predicted = logits.max(1)\n",
    "            test_correct += (predicted.indices == label_batch).sum().item()\n",
    "\n",
    "        print(f\"Avg Epoch Loss: {test_working_loss / test_total}, Accuracy: {test_correct / test_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04611786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1...\n",
      "Avg Epoch Loss: 1.2013620707914845, Accuracy: 0.7252\n",
      "Evaluating Epoch 1...\n",
      "Avg Epoch Loss: 0.33885960674285887, Accuracy: 0.749\n",
      "Training Epoch 2...\n",
      "Avg Epoch Loss: 0.7967201314616504, Accuracy: 0.8138\n",
      "Evaluating Epoch 2...\n",
      "Avg Epoch Loss: 0.271345986366272, Accuracy: 0.809\n",
      "Training Epoch 3...\n",
      "Avg Epoch Loss: 0.6779656310094432, Accuracy: 0.8434\n",
      "Evaluating Epoch 3...\n",
      "Avg Epoch Loss: 0.3037311329841614, Accuracy: 0.798\n",
      "Training Epoch 4...\n",
      "Avg Epoch Loss: 0.6330756516188865, Accuracy: 0.851\n",
      "Evaluating Epoch 4...\n",
      "Avg Epoch Loss: 0.20187254667282103, Accuracy: 0.803\n",
      "Training Epoch 5...\n",
      "Avg Epoch Loss: 0.5169731878939379, Accuracy: 0.8714\n",
      "Evaluating Epoch 5...\n",
      "Avg Epoch Loss: 0.22706766176223755, Accuracy: 0.852\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f\"Training Epoch {epoch}...\")\n",
    "\n",
    "    train_working_loss = 0\n",
    "    train_total = 0\n",
    "    train_correct = 0\n",
    "\n",
    "    model.train()\n",
    "    for image_batch, label_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(image_batch)\n",
    "        loss = loss_fn(logits, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_working_loss += loss.item()\n",
    "        train_total += len(image_batch)\n",
    "        predicted = logits.max(1)\n",
    "        train_correct += (predicted.indices == label_batch).sum().item()\n",
    "\n",
    "    print(f\"Avg Epoch Loss: {train_working_loss / train_total}, Accuracy: {train_correct / train_total}\")\n",
    "    \n",
    "    test_working_loss = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    print(f\"Evaluating Epoch {epoch}...\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image_batch, label_batch in test_dataloader:\n",
    "            logits = model(image_batch)\n",
    "            loss = loss_fn(logits, label_batch)\n",
    "\n",
    "            test_working_loss += loss.item()\n",
    "            test_total += len(image_batch)\n",
    "            predicted = logits.max(1)\n",
    "            test_correct += (predicted.indices == label_batch).sum().item()\n",
    "\n",
    "        print(f\"Avg Epoch Loss: {test_working_loss / test_total}, Accuracy: {test_correct / test_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fd5312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BriscClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "            nn.Dropout2d(p=0.2)\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "\n",
    "        dummy_ex = torch.randn((1, 3, 224, 224))\n",
    "        dummy_ex = self.conv_block1(dummy_ex)\n",
    "        dummy_ex = self.conv_block2(dummy_ex)\n",
    "        dummy_ex = self.conv_block3(dummy_ex)\n",
    "        dummy_ex = torch.flatten(dummy_ex, start_dim=1, end_dim=-1)\n",
    "\n",
    "        self.fc = nn.Linear(in_features=dummy_ex.size(1), out_features=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = torch.flatten(x, start_dim=1, end_dim=-1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0b4dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BriscClassification()\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d368a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1...\n",
      "Avg Epoch Loss: 0.35713809664044527, Accuracy: 0.7114\n",
      "Evaluating Epoch 1...\n",
      "Avg Epoch Loss: 0.10761877560615539, Accuracy: 0.8\n",
      "Training Epoch 2...\n",
      "Avg Epoch Loss: 0.19071498335464857, Accuracy: 0.8174\n",
      "Evaluating Epoch 2...\n",
      "Avg Epoch Loss: 0.06916032886505127, Accuracy: 0.816\n",
      "Training Epoch 3...\n",
      "Avg Epoch Loss: 0.1213436385545084, Accuracy: 0.8562\n",
      "Evaluating Epoch 3...\n",
      "Avg Epoch Loss: 0.08258754253387451, Accuracy: 0.8\n",
      "Training Epoch 4...\n",
      "Avg Epoch Loss: 0.08168597337187371, Accuracy: 0.8842\n",
      "Evaluating Epoch 4...\n",
      "Avg Epoch Loss: 0.0687889153957367, Accuracy: 0.791\n",
      "Training Epoch 5...\n",
      "Avg Epoch Loss: 0.09230625452424245, Accuracy: 0.8776\n",
      "Evaluating Epoch 5...\n",
      "Avg Epoch Loss: 0.04644046488404274, Accuracy: 0.861\n",
      "Training Epoch 6...\n",
      "Avg Epoch Loss: 0.05958367577162935, Accuracy: 0.903\n",
      "Evaluating Epoch 6...\n",
      "Avg Epoch Loss: 0.0307027205824852, Accuracy: 0.876\n",
      "Training Epoch 7...\n",
      "Avg Epoch Loss: 0.05116089496614854, Accuracy: 0.9074\n",
      "Evaluating Epoch 7...\n",
      "Avg Epoch Loss: 0.023062883615493773, Accuracy: 0.891\n"
     ]
    }
   ],
   "source": [
    "epochs = 7\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f\"Training Epoch {epoch}...\")\n",
    "\n",
    "    train_working_loss = 0\n",
    "    train_total = 0\n",
    "    train_correct = 0\n",
    "\n",
    "    model.train()\n",
    "    for image_batch, label_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(image_batch)\n",
    "        loss = loss_fn(logits, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_working_loss += loss.item()\n",
    "        train_total += len(image_batch)\n",
    "        predicted = logits.max(1)\n",
    "        train_correct += (predicted.indices == label_batch).sum().item()\n",
    "\n",
    "    print(f\"Avg Epoch Loss: {train_working_loss / train_total}, Accuracy: {train_correct / train_total}\")\n",
    "    \n",
    "    test_working_loss = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    print(f\"Evaluating Epoch {epoch}...\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image_batch, label_batch in test_dataloader:\n",
    "            logits = model(image_batch)\n",
    "            loss = loss_fn(logits, label_batch)\n",
    "\n",
    "            test_working_loss += loss.item()\n",
    "            test_total += len(image_batch)\n",
    "            predicted = logits.max(1)\n",
    "            test_correct += (predicted.indices == label_batch).sum().item()\n",
    "\n",
    "        print(f\"Avg Epoch Loss: {test_working_loss / test_total}, Accuracy: {test_correct / test_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc58065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
